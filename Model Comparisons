import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

def engagement_model():
    rec = None
    
    from sklearn.model_selection import train_test_split
    training_data = pd.read_csv('assets/train.csv').set_index('id')
    test_data = pd.read_csv('assets/test.csv').set_index('id')
    X = training_data.iloc[:,:-1]
    y = training_data.iloc[:,-1]
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
        
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.fit_transform(X_test)
    test_scaled = scaler.fit_transform(test_data)
    
    from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(degree=2)
    X_train_poly_scaled = poly.fit_transform(X_train_scaled)
    X_test_poly_scaled = poly.fit_transform(X_test_scaled)
    test_poly_scaled = poly.fit_transform(test_scaled)
    
    from sklearn.linear_model import LogisticRegression
    from sklearn.neural_network import MLPClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.svm import SVC
    
    from sklearn.model_selection import GridSearchCV
    from sklearn.metrics import roc_auc_score
    
    # logreg = LogisticRegression(solver='liblinear').fit(X_train_poly_scaled, y_train)
    # grid_params = {'penalty': ['l1', 'l2'], 'C':[0.01, 0.1, 1, 10]}
    # gs_eval = GridSearchCV(logreg, param_grid=grid_params, scoring='roc_auc', cv=5).fit(X_test_poly_scaled, y_test)
    # best_logreg_auc_idx = gs_eval.cv_results_['mean_test_score'].max(), np.argmax(gs_eval.cv_results_['mean_test_score'])
    # best_logreg_model = gs_eval.cv_results_['params'][best_logreg_auc_idx[1]]
    # return f'Tuple {best_logreg_auc_idx} and Model {best_logreg_model}'

    logreg = LogisticRegression(solver='liblinear', C=10, penalty='l1').fit(X_train_poly_scaled, y_train)
    ans = pd.DataFrame(data=logreg.predict_proba(test_poly_scaled), columns=['unengaged', 'engagement'])
    ans.index = test_data.index
    return ans['engagement']
    
    # gaussian_fit = GaussianNB().fit(X_train_scaled, y_train)
    # return roc_auc_score(y_test, gaussian_fit.predict_proba(X_test_scaled))
    # grid_params = {'penalty': ['l1', 'l2'], 'C':[0.01, 0.1, 1, 10]}
    # gs_eval = GridSearchCV(logreg, param_grid=grid_params, scoring='roc_auc', cv=5).fit(X_test_poly_scaled, y_test)
    # best_logreg_auc_idx = gs_eval.cv_results_['mean_test_score'].max(), np.argmax(gs_eval.cv_results_['mean_test_score'])
    # best_logreg_model = gs_eval.cv_results_['params'][best_logreg_auc_idx[1]]
    # return f'Tuple {best_logreg_auc_idx} and Model {best_logreg_model}'

    # for clas, param in []
    #     if param == []
    
    mlp = (MLPClassifier(hidden_layer_sizes = [10, 10], solver='lbfgs', alpha = 0.1, activation = this_activation,
                 random_state = 0).fit(X_train_scaled, y_train))
    
    knnfit = KNeighborsClassifier(n_neighbors = 2).fit(X_train, y_train)
    
    tree_fit = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train) 
    
    random_forest_fit = RandomForestClassifier(max_features=8, n_estimators=100, random_state = 0).fit(X_train, y_train)
    
    gbdt = (GradientBoostingClassifier(learning_rate = 0.01, n_estimators=25, max_depth = 2, random_state = 0)
                .fit(X_train, y_train))
    
    scv_linear_fit = SVC(kernel = 'linear', C=1.0).fit(X_train_scaled, y_train)
    scv_poly_fit = SVC(kernel = 'poly', C=1.0, deg=3).fit(X_train_scaled, y_train)
    scv_rbf_fit = SVC(kernel = 'rbf', C=1.0, gamma=1.0).fit(X_train_scaled, y_train)


    return test_data
    return model 
    return rec

engagement_model()
